{
  "hash": "fd01f540dea05c4e62b2c59c25610dd5",
  "result": {
    "markdown": "---\ntitle: But how do we evaluate classifiers?\n---\n\nSo you have decided to classify stuff i.e. you have a labelled dataset and want your machine learning model to classify the dataset into desired labels. This is wonderful. If you are familiar with Sci-kit learn you can easily fire up few models to do your job. But this begs the question, what is the best model to use for your particular task. And no, it is not the most fancy model with out of the world complexity although it does look like a very attractive option. So how do we decide what is the best? Let's find out together.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#importing necessary libraries\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n```\n:::\n\n\n## Load up the dataset\n\nWe can use any dataset in our disposal. scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. So, we will use one of them to show how to evaluate the models we build. We will keep life simple and flowery!\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\niris = datasets.load_iris()\n```\n:::\n\n\nNow that we have the dataset, first thing we will do is assign it to target and variables.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nx=iris.data\ny=iris.target\nprint(x[0:3])\nprint(y[0:3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]]\n[0 0 0]\n```\n:::\n:::\n\n\nNot very descriptive is it? This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. You can find details here : https://scikit-learn.org/stable/datasets/toy_dataset.html\n\nWe can now find the attributes information\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(\"The features are: \",list(iris.feature_names))\nprint(\"The target classes are: \", list(iris.target_names))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe features are:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nThe target classes are:  ['setosa', 'versicolor', 'virginica']\n```\n:::\n:::\n\n\nSo, X is an array of the features respectively and y have the class attributes. For example, **0** stands for **setosa**, **1** stands for **versicolor** and so on. You get the idea! We will use the features to predict the class of the sample flower. Told you it is going to be flowery!\n\n## Training and Testing split\n\nAs usual, we need a dataset to teach the model the pattern, the **Training Set** and another to test how good a student it really is, the **Testing Set**.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n# 70% for training and 30% for testing\nX_train, X_test, y_train, y_test = train_test_split(x, y,test_size=.3, random_state=42) #The answer to everythin  in the universe is 42, use it.\n```\n:::\n\n\n## Lets fire up the models\n\n Now we get to choose the models we want to evaluate, For demonstration purposes, I am choosing 3 models - **SGD Classifier**, **Decision Tree Classifier** and **Random Forest Classifier**. You can choose any models you want but the framework will be same.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#importing the models\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nsgd_model = SGDClassifier(random_state = 42)\ndt_model = DecisionTreeClassifier(random_state = 42)\nrf_model = RandomForestClassifier(random_state = 42)\n```\n:::\n\n\nWe have the models. First, we can use the Accuracy metric to see how all three can perform.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\nmodels = {\n    \"SGD \": sgd_model,\n    \"Decision Tree \": dt_model,\n    \"Random Forest\": rf_model\n}\ndef model_accuracies(models, X_train, y_train, X_test, y_test):\n    for model_name, model in models.items():\n        \n        model.fit(X_train, y_train)   \n        y_pred = model.predict(X_test)\n     \n        accuracy = accuracy_score(y_test, y_pred)\n        \n        print(f\"{model_name}: {accuracy:.2f}\")\nmodel_accuracies(models, X_train, y_train, X_test, y_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSGD : 0.80\nDecision Tree : 1.00\nRandom Forest: 1.00\n```\n:::\n:::\n\n\nWhoa! Would you look at that! But seriously life can not be that good, right? We can not trust the model yet. For a better understanding, we need to look at something called **Cross-Validation**\n\n## Cross-Validation\n\nWe split the data into k folds i.e. k number of non overlapping subsets. In this case, lets do 3 folds to keep it simple. Another concept is using a **Dummy Classifier**. It does not look at the features. In concept, your model has to surpass the dummy classifier to be considered a good model.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier() \n\ndef cv_score(models,X_train, y_train):\n    for model_name, model in models.items():\n        cv_score = cross_val_score(model,X_train, y_train, cv=3, scoring=\"accuracy\").mean().round(3)\n        print(f\"{model_name}: {cv_score}\")\n        \ncv_score(models,X_train, y_train)\n\nprint(\"Dummy Classifier: \", cross_val_score(dummy_clf ,X_train, y_train, cv=3, scoring=\"accuracy\").mean().round(3))\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSGD : 0.924\nDecision Tree : 0.895\nRandom Forest: 0.933\nDummy Classifier:  0.343\n```\n:::\n:::\n\n\nAlright, now the SGD model does not look too bad, does it? But there is another check we need to do. That is called **Confusion Matrix**\n\n## Confusion Matrix\n\nSimply put, it is a visual tool to undestand how confused the model is when predicting the classes. Or we can say a tool to understand how much the model is misclassifying each classes. We can generate the confusin matrix for each model below-\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nclass_names = list(iris.target_names)\ndef plot_confusion_matrices(models, X_train, y_train, X_test, y_test, class_names):\n\n   \n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n    axes = axes.flatten()  \n    fig.delaxes(axes[3])  \n\n    for i, (model_name, model) in enumerate(models.items()):\n        \n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        sgd_cm = confusion_matrix(y_test, y_pred)\n        \n        sns.heatmap(sgd_cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')\n        axes[i].set_title(model_name)\n        axes[i].set_xlabel('Predicted')\n        axes[i].set_ylabel('Actual')\n        axes[i].set_xticklabels(class_names)\n        axes[i].set_yticklabels(class_names)\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_confusion_matrices(models, X_train, y_train, X_test, y_test, class_names)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=940 height=758}\n:::\n:::\n\n\nAs we can see in the figure, SGD had 8 instances where it misclassified **Versicolor** as **Virginica**. So our choice comes to the Decision Tree and Random Forest. Now we did a multiclass classification. Two important metrics are **recall** and **precision**. This concept is better understood in binary classfification. We can make our multiclass problem into binary problem by selecting a class. Maybe we can go with the class **Versicolor**. So, our models are going to predict if an instance represents versicolor or not.  \n\n## Recall and Precision\n\nTo understand recall and precision we need to understand few terms from a binary confusion matrix. So first, we can generate one for SGD after binarizing the data.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import label_binarize\ny_bin = label_binarize(y, classes=[0, 1, 2])[:, 1]\n\nX_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(x, y_bin,test_size=.3, random_state=42)\n\nsgd_model.fit(X_train_bin, y_train_bin)\ny_pred_bin = sgd_model.predict(X_test_bin)\n\nsgd_cm = confusion_matrix(y_test_bin, y_pred_bin)\n\n# Extracting the values from the confusion matrix\nTP = sgd_cm[1, 1]  # True Positives\nTN = sgd_cm[0, 0]  # True Negatives\nFP = sgd_cm[0, 1]  # False Positives\nFN = sgd_cm[1, 0]  # False Negatives\n\nprint(\"True Positives: \", TP)\nprint(\"True Negatives: \", TN)\nprint(\"False Positives: \", FP)\nprint(\"False Negatives: \", FN)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTrue Positives:  1\nTrue Negatives:  32\nFalse Positives:  0\nFalse Negatives:  12\n```\n:::\n:::\n\n\n## Key Attributes of a Confusion Matrix\n\n- **True Positives (TP)**: These are cases in which the model correctly predicts the positive class. In other words, instances where the actual class is positive, and the model also predicts a positive class.\n\n- **True Negatives (TN)**: These are cases where the model correctly predicts the negative class. This means the actual class is negative, and the model also predicts a negative class.\n\n- **False Positives (FP)**: These are instances where the model incorrectly predicts the positive class. This happens when the actual class is negative, but the model predicts a positive class. It is also known as a \"Type I error\".\n\n- **False Negatives (FN)**: These are cases where the model incorrectly predicts the negative class. In these instances, the actual class is positive, but the model predicts a negative class. This is also referred to as a \"Type II error\".\n\n### Precision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations. It is a measure of a classifier's exactness. A low precision indicates a high number of false positives. The precision is defined as:\n\n\n$$ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} $$\n\n- **TP (True Positives)**: The number of positive instances correctly identified by the model.\n- **FP (False Positives)**: The number of negative instances incorrectly identified as positive by the moel.\ns.\n\nPrecision is particularly important in scenarios where the cost of a false positive is high. For example, in email spam detection, a false positive (non-spam email incorrectly classified as spam) could mean missing an important email.\n\n### Recall\nRecall, also known as sensitivity, is the ratio of correctly predicted positive observations to all observations in the actual class. It is a measure of a classifier's completeness. A low recall indicates a high number of false negatives. The recall is defi$$ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\n\n- **TP (True Positives)**: The number of positive instances correctly identified by the model.\n- **FN (False Negatives)**: The number of positive instances incorrectly identified as negative by the model.egatives.\n\nRecall becomes critical in cases where missing an actual positive is significantly worse than getting false positives. For instance, in medical diagnosis, a false negative (missing a disease) could be much more detrimental the positive.an a fals\n\n\n## Precision-Recall Curve\nThe Precision-Recall Curve is a plot that illustrates the trade-off between precision and recall for different threshold settings of a classifier. It is used as a tool to select models that balance precision and recall in a desirable way, especially in scenarios with imbalanced datasets.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\nfrom sklearn.model_selection import cross_val_predict\n\ny_probas_dt = cross_val_predict(dt_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"predict_proba\") \ny_scores_dt = y_probas_dt[:, 1]\nprecisions_dt, recalls_dt, thresholds_dt = precision_recall_curve(y_train_bin, y_scores_dt)\n\ny_scores_sgd= cross_val_predict(sgd_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"decision_function\")\n\nprecisions_sgd, recalls_sgd, thresholds_sgd = precision_recall_curve(y_train_bin, y_scores_sgd)\n\ny_probas_rf = cross_val_predict(rf_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"predict_proba\")\ny_scores_rf = y_probas_rf[:, 1]\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_bin, y_scores_rf)\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nplt.figure(figsize=(6, 5))  \n\nplt.plot(recalls_rf, precisions_rf, \"b-\", linewidth=2,\n         label=\"Random Forest\")\nplt.plot(recalls_sgd, precisions_sgd, \"--\", linewidth=2, label=\"SGD\")\nplt.plot(recalls_dt, precisions_dt, \"--\", linewidth=2, label=\"Decision Tree\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=525 height=434}\n:::\n:::\n\n\nNow in a precision recall curve, we look at the top right of the curve. Best model for the purpose will be in that region. That means higher precision with higher recall. The interpretation of the results can be explaines as-\n\n\n- **Random Forest (solid blue line)**: This model has the best performance among the three. It maintains a high precision even as recall increases, which means it correctly identifies a high percentage of positive cases and, when it predicts a case to be positive, it is likely correct.\n\n- **SGD (dashed line)**: The performance of the SGD model varies more than the Random Forest. Its precision starts lower and fluctuates as recall increases. This suggests that the model may not be as reliable in its positive predictions compared to the Random Forest.\n\n- **Decision Tree (dashed green line)**: The Decision Tree's performance is generally lower than the Random Forest. While it has moments where its precision is high, it also has points where precision drops significantly as recall increases. This might indicate that the Decision Tree model, at certain thresholds, predicts many false positives while trying to capture more true positives.\n\nSo there we have it, **Random Forest** can be used for the job at hand! Thank you for reading the blog\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
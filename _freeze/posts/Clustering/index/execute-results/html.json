{
  "hash": "a31685b9ce572dd3104a7f093f6559f1",
  "result": {
    "markdown": "---\ntitle: Introduction to Unsupervised Machine Learning Through Clustering\n---\n\n## So What is Clustering Anyway?\n\nYou are on your first day at your job and have just been handed a huge amount of data about your customers. You have been tasked to identify the buying groups your company can direct the advertisement of products to. \"Well, that is a mammoth task\"-you say to yourself wondering how you can do it. Not to worry, you are familiar with machine learning. You can let an algorithm identify them for you. This task is called **Clustering** and our friend unsupervised machine learning is here to help you with that. In clustering, the goal is to group similar instances into clusters. \n\n## The Game Plan\n\nWe look at the IRIS dataset. It is a comprehensive dataset with 3 classes. We can do a simple classification problem. But let's go another way. We will forget that the data is labeled. By clustering, we will see if our machine learning model can by itself create the classes. So, until the end, we are going in blind. And the interesting thing about going in blind, we do not have to do data splitting because there will be no traditional training and testing.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\niris = datasets.load_iris(as_frame = True)\nX = iris.data\ny = iris.target\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#plotting the classes\ndata = X.copy()\ndata['Cluster'] = y\n\n# Mapping numerical labels to actual species names\nspecies_map = {0: 'Iris setosa', 1: 'Iris versicolor', 2: 'Iris virginica'}\ndata['Cluster'] = data['Cluster'].map(species_map)\n\n# Plotting\nplt.figure(figsize=(7, 5))\nsns.scatterplot(data=data, x='petal length (cm)', y='petal width (cm)', hue='Cluster',  style='Cluster', markers=['o', 's', '^'])\nplt.title(\"Clustering based on labels\")\nplt.grid(True)\nplt.legend(title='Species')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=449}\n:::\n:::\n\n\nThis is what the plot scattered plot looks like in a two din=mensional space. I am only measuring in terms of petal length and width with the assumption that these two features will make the division more clear. And we can see the natural clustering with known labels in the plot. \n\n## K-Means Clusturing\n\nImagine you want to group a collection of points (or data points) into clusters. Each cluster should have a central point known as a \"centroid.\" The goal is to ensure each point in a cluster is close to its centroid. The challenge is that you initially don't know where the centroids should be or which points belong to which cluster.\n\n**Steps to Cluster Data**\n\n**1. Start by Guessing:**  \n- Begin by making a guess about where the centroids might be.\n- You can do this randomly, such as by picking a few points from your data and using them as your initial centroids.\n\n**2. Assign Points to the Nearest Centroid:**  \n- Examine each point in your data.\n- Assign it to the cluster of the closest centroid you guessed.\n- This forms your initial clusters.\n\n**3. Update Centroids:**  \n- Once you have your initial clusters, find the new center for each cluster.\n- The new center is the average position of all points in the cluster.\n- These average positions become your new centroids.\n\n**4. Repeat:**  \n- With these new centroids, reassign each point to the closest centroid.\n- This step may shuffle some points between clusters.\n\n**5. Keep Going Until Stabilization:**  \n- Continue updating the centroids and reassigning points.\n- Stop the process when the centroids do not change significantly anymore.\n- At this point, you have found a stable set of clusters.\n\nThis method is a basic implementation of a popular clustering algorithm known as \"K-Means.\" It's a straightforward yet effective way to group data points into clusters when you don't have predefined labels or categories. The algorithm is designed to converge to a solution in a reasonable amount of time, ensuring each iteration improves the clustering.\n\n## 'K' in K-means Clusturing: Why Do We Need to Optimize?\n\nIn K-means clusturing, we have to first set the number of clusters to be made during the initialization of the model. If we make too many clusters, the computatinal time will increase and too few will not be useful. But we have no metrics to test the data against real labels so how do we do that?.\n\n## Inertia\n\nInertial is defined as the  the sum of the squared distances between the instances and their closest centroids. So lets run a loop and produce the inertia for different number of clusters. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nfrom timeit import timeit\n\nmax_k = 10\ndata = {\n    'k': [],\n    'Inertia': [],\n    'Training Time': []\n}\n\nfor k in range(1, max_k + 1):\n    kmeans_ = KMeans(n_clusters=k, algorithm=\"full\", random_state=42)\n    print(f\"\\r{k}/{max_k}\", end=\"\")  # \\r returns to the start of line\n    training_time = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n    inertia = kmeans_.inertia_\n\n    data['k'].append(k)\n    data['Inertia'].append(inertia)\n    data['Training Time'].append(training_time)\n\ndf = pd.DataFrame(data)\n\nsns.set_style(\"whitegrid\")\n\nplt.figure(figsize=(8, 4))\n\n# Inertia plot\nplt.subplot(121)\nsns.lineplot(x='k', y='Inertia', data=df, marker='o', color='red', label=\"K-Means\")\nplt.title(\"Inertia vs. k\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\n# Training time plot\nplt.subplot(122)\nsns.lineplot(x='k', y='Training Time', data=df, marker='o', color='blue', label=\"K-Means\")\nplt.title(\"Training Time vs. k\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Training Time (seconds)\")\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\r1/10\r2/10\r3/10\r4/10\r5/10\r6/10\r7/10\r8/10\r9/10\r10/10\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=758 height=372}\n:::\n:::\n\n\nAs we can see from the plots, the inertial decreases with increased clusters. That is a good thing! But look the training time. This is increasing much more than we can take. Why? Remember your model will not be scalable if it takes up too much resources. But maybe we can find some way, When we plot inertia as a function of k, after some point, we see the gradient not being very high i.e. inertia is not decreasing that much. This point can look like an elbow, and the elbow joint can lead us to the optimum clusters. Maybe not fullproof but this is a step in the right direction. This method is evidently called **Elbow Method**.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(6, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\nplt.axis([1, 9, 0, max(inertias) + 100])  # Adjusted the axis for better visibility\nplt.title(\"Elbow Method for Optimal $k$\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=560 height=317}\n:::\n:::\n\n\nCan you notice the elbow like joint when k = 2? maybe 3? now we can identify the region but still there is another good metrics to find out.\n\n## Silhouette Score\n\nAn instance’s silhouette coefficient is equal to \n$$\\frac{b - a}{\\max(a, b)}$$\nwhere **a** is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and\n**b** is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instance’s own cluster).\n\nThe silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(6, 3.5))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\n\nplt.grid(True, which='both', linestyle='-', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=560 height=319}\n:::\n:::\n\n\nso when k=2, the silhoutte score is highest. Maybe then the data can be divided into two clusters. But hold your horses. We have one more thing to do. We need to see the **Silhoutte Diagram**!\n\n## Silhouette Diagram\n\nA more detailed visual representation can be achieved by plotting the silhouette coefficient of each instance. These plots are organized by the cluster assignment and the coefficient values. Such visualizations are known as silhouette diagrams. Each cluster in the diagram is represented by a 'knife-like' shape, where the height shows how many instances are in the cluster, and the width indicates the silhouette coefficients of the instances, sorted within the cluster. In these diagrams, wider shapes are indicative of better clustering.\n\nThe average silhouette score for each potential number of clusters is marked by vertical dashed lines in the diagram. If the silhouette coefficients of most instances in a cluster are less than this average score, meaning that the knife shapes fall short of the dashed line, it suggests that the cluster is not well-defined. This happens when the instances are too close to instances in other clusters, implying a less than optimal clustering arrangement.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(8, 6))\n\nfor k in (2,3, 4, 5):\n    plt.subplot(2, 2, k - 1)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (2, 4):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (4, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=655 height=529}\n:::\n:::\n\n\n## Interpetting the Silhoutte Diagram\n\n- **For `k=2`**: This setup shows two clusters with a fairly good silhouette width for the larger cluster. This suggests a decent separation between clusters. However, the simplicity of having only two clusters might not capture all the nuances in the data.\n\n- **For `k=3`**: Increasing the cluster count to three shows more uniformity in silhouette widths, indicating that the clusters might be more distinct and appropriately separated. Yet, some instances below the average silhouette score signal potential overlap between clusters.\n\n- **For `k=4`**: Here, the silhouette widths start to vary more significantly, implying different levels of cohesion within the clusters. The presence of lower silhouette scores within some clusters indicates potential misplacements or that these clusters are not capturing a distinct grouping in the data.\n\n- **For `k=5`**: With five clusters, we notice even more variation in silhouette widths, with generally narrower shapes compared to `k=3`. This suggests that the additional cluster may not be necessary as it does not seem to capture distinct groupings, and some points could potentially belong to other clusters.\n\nBased on this analysis, `k=3` may be the most appropriate number of clusters for this dataset, as the clusters display a balance of separation and uniformity in silhouette widths. As the number of clusters increases to 4 and 5, the silhouette diagrams indicate a diminishing return in cluster definition and separation.\n\n## Overall Comparison With the Actual Labels\n\nFinally we can now create three 2D spaces for actual labels and see how the clusters show the real labels. We are doing this just because we pretended we do not have actual labels so now we can compare!\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters=3, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\n# Create a mapping based on modes\nmapping = {}\nfor class_id in np.unique(y):\n    mode, _ = stats.mode(y_pred[y == class_id])\n    if np.isscalar(mode):\n        mapping_key = mode\n    else:\n        mapping_key = mode[0] if mode.size > 0 else -1\n    mapping[mapping_key] = class_id\n\ny_pred = np.array([mapping.get(cluster_id, -1) for cluster_id in y_pred])\n\ndata = X.copy()\ndata['Predicted'] = y_pred\ndata['Actual'] = y\n\nlabel_names = {0: 'Iris setosa', 1: 'Iris versicolor', 2: 'Iris virginica'}\ndata['Actual Names'] = data['Actual'].map(label_names)\n\n# Plotting using FacetGrid\nplt.figure(figsize=(6, 4))\ng = sns.FacetGrid(data, col=\"Actual Names\", hue=\"Predicted\", palette=\"bright\", height=4, aspect=1)\ng.map(plt.scatter, 'petal length (cm)', 'petal width (cm)', alpha=0.6, s=100)\ng.add_legend()\ng.set_titles(\"{col_name}\")\n\nfor ax in g.axes.flatten():\n    ax.grid(True)  \n    ax.set_xlabel(\"Petal length\")\n    ax.set_ylabel(\"Petal width\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 576x384 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-2.png){width=1221 height=367}\n:::\n:::\n\n\nSo our dear to heart K-Means model did made some error. But remember \"To err is human\". See what I did there? Good luck!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
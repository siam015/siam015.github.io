{
  "hash": "4c759a4c2f1910ff7cb4bd27e92358b0",
  "result": {
    "markdown": "---\ntitle: Mechanism of Regression\n---\n\n## But What is Regression?\n\nWelcome to the fascinating world of predictive analytics! Today, we're diving into one of its most vital tools: regression. Regression isn't just a statistical method; it's the secret sauce behind forecasting outcomes with stunning precision. Picture this: you're trying to guess the value of a house. How do you do it? You look at its size, the number of bedrooms, location, and a whole bunch of other details. This is where regression shines!\n\n## Linear Regression\n\nLinear regression is a popular regression learning algorithm that learns a model which is a\nlinear combination of features of the input. It attemts to minimize the difference (not really but we will get into that) between its prediction and target values. Now, if I ask you to learn something by trial and error, you will then ask me well how do I know if I am getting closer to actual solution? Any regression algorithm needs to evaluate its performance during training period. So, during training a linear algorithm, we will set its **objective** to minimize error defined as a  **loss function** or **cost function**. \n\nWe can use a little bit of mathematics to understand the concept of linear regression. A linear regression defines its output $y$ given the feature $x1$ and $x2$ as - \n$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 $$\nWhere $\\beta_0$ is the intercept and \n\n$\\beta_1$ and $\\beta_2$ are weights\n\nFor this linear regression equation  the cost function is typically defined as the Mean Squared Error (MSE). The MSE measures the average of the squares of the errors, which are the differences between the observed values and the values predicted by the linear model. The cost function can be written as:\n\n$$ J(\\beta_0, \\beta_1, \\beta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}))^2 $$\n\nHere:\n\n- $J(\\beta_0, \\beta_1, \\beta_2)$ is the cost function.\n- $n$ is the number of observations in the dataset.\n- $ y_i$ is the actual observed value of the dependent variable for the \\( i \\)-th observation.\n- $x_{1i}$ and $ x_{2i}$ are the values of the first and second independent variables for the $ ith$ observation.\n\n\nThe goal in training the linear regression model is to find the values of $\\beta_0$, $\\beta_1$, and $\\beta_2$ that minimize this cost function.\n\n\n## Lets see it in action\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generating synthetic data\nx1 = np.random.rand(100, 1)  # feature 1\nx2 = np.random.rand(100, 1)  # feature 2\ny = 1 + 2*x1 + 3*x2 + np.random.randn(100, 1)  # target variable with some noise\n\n\nX = np.hstack((x1, x2))\n\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Coefficients (beta_1 and beta_2) and intercept (beta_0)\nbeta_0 = model.intercept_[0]\nbeta_1, beta_2 = model.coef_[0]\n\n\ny_pred = model.predict(X)\n\n# Calculating the Mean Squared Error (MSE)\nmse = mean_squared_error(y, y_pred)\n\n\nequation = f\"y = {beta_0:.2f} + {beta_1:.2f}x1 + {beta_2:.2f}x2\"\nmse_text = f\"MSE: {mse:.2f}\"\n\nequation, mse_text\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n('y = 0.81 + 1.95x1 + 3.57x2', 'MSE: 0.94')\n```\n:::\n:::\n\n\nThe trained linear regression model based on the synthetic data is represented by the equation:\n\n$$ y = 1.08+ 1.72x_1 + 3.03x_2 $$\n\nThe Mean Squared Error (MSE), which is the average of the squares of the errors between the observed values and the values predicted by the model, for this training is:\n\n$$ \\text{MSE} = 0.93 $$\n\nThis MSE value reflects the average squared difference between the predicted values and the actual values in the dataset, and the objective in training is to minimize this error.\n\nIf we plot the regression:\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\n\n# Using seaborn's regplot for plotting\nplt.figure(figsize=(7, 5))\nsns.regplot(x=y.flatten(), y=y_pred.flatten(), color='purple', line_kws={'label': 'Regression Line'})\nplt.title('Actual vs Predicted y' )\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.legend()\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=576 height=449}\n:::\n:::\n\n\n## Decision Tree\n\nDecision Tree is a  nonlinear algorithm mainly for classification but we can repurpose it for regression analysis.A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature j of the feature vector is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. We can showcase the generated tree below:\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nimport graphviz\nfrom graphviz import Source\n\ndt_model = DecisionTreeRegressor(max_depth=2)\ndt_model.fit(X, y)\n\ndot_data = export_graphviz(dt_model, out_file=\"Tree.dot\", \n                           feature_names=['x1', 'x2'],  \n                           filled=True, rounded=True,  \n                           special_characters=True)\n\nSource.from_file(\"Tree.dot\")\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n![](index_files/figure-html/cell-4-output-1.svg){}\n:::\n:::\n\n\n## How Our Decision Tree Works\n\nLet's break down how a decision tree sorts data. We start with a dataset and we want to make predictions or decisions based on this data. Here's the step-by-step process:\n\n**Starting at the Top**\n\nWe begin with all our data at the root node.\n\n**First Question - Evaluating `x2`**\n\n- **Is `x2` less than or equal to 0.471?**\n  - **Yes:** Move to the left child node.\n  - **No:** Move to the right child node.\n\n**Going Left - Further Evaluation of `x1`**\n\n- **Now, is `x1` less than or equal to 0.72?** \n  - This node is further split into two branches based on this condition.\n\n**Going Right - Evaluating `x1` Again**\n\n- **Is `x1` less than or equal to 0.303?**\n  - Similarly, this node is split based on the condition.\n\n**Ending Up at a Leaf**\n\n- The leaf nodes are the endpoints where predictions are made based on the data that reached the leaf.\n\nAt the end of this process, our data is segmented into groups that help us make informed decisions or predictions.\nmaking predictions with our data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}
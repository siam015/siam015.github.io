[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html",
    "title": "Mechanism of Regression",
    "section": "",
    "text": "Welcome to the fascinating world of predictive analytics! Today, we’re diving into one of its most vital tools: regression. Regression isn’t just a statistical method; it’s the secret sauce behind forecasting outcomes with stunning precision. Picture this: you’re trying to guess the value of a house. How do you do it? You look at its size, the number of bedrooms, location, and a whole bunch of other details. This is where regression shines!"
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html#but-what-is-regression",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html#but-what-is-regression",
    "title": "Mechanism of Regression",
    "section": "",
    "text": "Welcome to the fascinating world of predictive analytics! Today, we’re diving into one of its most vital tools: regression. Regression isn’t just a statistical method; it’s the secret sauce behind forecasting outcomes with stunning precision. Picture this: you’re trying to guess the value of a house. How do you do it? You look at its size, the number of bedrooms, location, and a whole bunch of other details. This is where regression shines!"
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html#linear-regression",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html#linear-regression",
    "title": "Mechanism of Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nLinear regression is a popular regression learning algorithm that learns a model which is a linear combination of features of the input. It attemts to minimize the difference (not really but we will get into that) between its prediction and target values. Now, if I ask you to learn something by trial and error, you will then ask me well how do I know if I am getting closer to actual solution? Any regression algorithm needs to evaluate its performance during training period. So, during training a linear algorithm, we will set its objective to minimize error defined as a loss function or cost function.\nWe can use a little bit of mathematics to understand the concept of linear regression. A linear regression defines its output \\(y\\) given the feature \\(x1\\) and \\(x2\\) as - \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] Where \\(\\beta_0\\) is the intercept and\n\\(\\beta_1\\) and \\(\\beta_2\\) are weights\nFor this linear regression equation the cost function is typically defined as the Mean Squared Error (MSE). The MSE measures the average of the squares of the errors, which are the differences between the observed values and the values predicted by the linear model. The cost function can be written as:\n\\[ J(\\beta_0, \\beta_1, \\beta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}))^2 \\]\nHere:\n\n\\(J(\\beta_0, \\beta_1, \\beta_2)\\) is the cost function.\n\\(n\\) is the number of observations in the dataset.\n$ y_i$ is the actual observed value of the dependent variable for the ( i )-th observation.\n\\(x_{1i}\\) and $ x_{2i}$ are the values of the first and second independent variables for the $ ith$ observation.\n\nThe goal in training the linear regression model is to find the values of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) that minimize this cost function."
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html#lets-see-it-in-action",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html#lets-see-it-in-action",
    "title": "Mechanism of Regression",
    "section": "Lets see it in action",
    "text": "Lets see it in action\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Generating synthetic data\nx1 = np.random.rand(100, 1)  # feature 1\nx2 = np.random.rand(100, 1)  # feature 2\ny = 1 + 2*x1 + 3*x2 + np.random.randn(100, 1)  # target variable with some noise\n\n\nX = np.hstack((x1, x2))\n\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Coefficients (beta_1 and beta_2) and intercept (beta_0)\nbeta_0 = model.intercept_[0]\nbeta_1, beta_2 = model.coef_[0]\n\n\ny_pred = model.predict(X)\n\n# Calculating the Mean Squared Error (MSE)\nmse = mean_squared_error(y, y_pred)\n\n\nequation = f\"y = {beta_0:.2f} + {beta_1:.2f}x1 + {beta_2:.2f}x2\"\nmse_text = f\"MSE: {mse:.2f}\"\n\nequation, mse_text\n\n('y = 0.81 + 1.95x1 + 3.57x2', 'MSE: 0.94')\n\n\nThe trained linear regression model based on the synthetic data is represented by the equation:\n\\[ y = 1.08+ 1.72x_1 + 3.03x_2 \\]\nThe Mean Squared Error (MSE), which is the average of the squares of the errors between the observed values and the values predicted by the model, for this training is:\n\\[ \\text{MSE} = 0.93 \\]\nThis MSE value reflects the average squared difference between the predicted values and the actual values in the dataset, and the objective in training is to minimize this error.\nIf we plot the regression:\n\nimport seaborn as sns\n\n# Using seaborn's regplot for plotting\nplt.figure(figsize=(7, 5))\nsns.regplot(x=y.flatten(), y=y_pred.flatten(), color='purple', line_kws={'label': 'Regression Line'})\nplt.title('Actual vs Predicted y' )\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html#decision-tree",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html#decision-tree",
    "title": "Mechanism of Regression",
    "section": "Decision Tree",
    "text": "Decision Tree\nDecision Tree is a nonlinear algorithm mainly for classification but we can repurpose it for regression analysis.A decision tree is an acyclic graph that can be used to make decisions. In each branching node of the graph, a specific feature j of the feature vector is examined. If the value of the feature is below a specific threshold, then the left branch is followed; otherwise, the right branch is followed. As the leaf node is reached, the decision is made about the class to which the example belongs. We can showcase the generated tree below:\n\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nimport graphviz\nfrom graphviz import Source\n\ndt_model = DecisionTreeRegressor(max_depth=2)\ndt_model.fit(X, y)\n\ndot_data = export_graphviz(dt_model, out_file=\"Tree.dot\", \n                           feature_names=['x1', 'x2'],  \n                           filled=True, rounded=True,  \n                           special_characters=True)\n\nSource.from_file(\"Tree.dot\")"
  },
  {
    "objectID": "posts/Linear-And-Nonlinear-Regression/index.html#how-our-decision-tree-works",
    "href": "posts/Linear-And-Nonlinear-Regression/index.html#how-our-decision-tree-works",
    "title": "Mechanism of Regression",
    "section": "How Our Decision Tree Works",
    "text": "How Our Decision Tree Works\nLet’s break down how a decision tree sorts data. We start with a dataset and we want to make predictions or decisions based on this data. Here’s the step-by-step process:\nStarting at the Top\nWe begin with all our data at the root node.\nFirst Question - Evaluating x2\n\nIs x2 less than or equal to 0.471?\n\nYes: Move to the left child node.\nNo: Move to the right child node.\n\n\nGoing Left - Further Evaluation of x1\n\nNow, is x1 less than or equal to 0.72?\n\nThis node is further split into two branches based on this condition.\n\n\nGoing Right - Evaluating x1 Again\n\nIs x1 less than or equal to 0.303?\n\nSimilarly, this node is split based on the condition.\n\n\nEnding Up at a Leaf\n\nThe leaf nodes are the endpoints where predictions are made based on the data that reached the leaf.\n\nAt the end of this process, our data is segmented into groups that help us make informed decisions or predictions. making predictions with our data."
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "But how do we evaluate classifiers?",
    "section": "",
    "text": "So you have decided to classify stuff i.e. you have a labelled dataset and want your machine learning model to classify the dataset into desired labels. This is wonderful. If you are familiar with Sci-kit learn you can easily fire up few models to do your job. But this begs the question, what is the best model to use for your particular task. And no, it is not the most fancy model with out of the world complexity although it does look like a very attractive option. So how do we decide what is the best? Let’s find out together.\n#importing necessary libraries\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Classification/index.html#load-up-the-dataset",
    "href": "posts/Classification/index.html#load-up-the-dataset",
    "title": "But how do we evaluate classifiers?",
    "section": "Load up the dataset",
    "text": "Load up the dataset\nWe can use any dataset in our disposal. scikit-learn comes with a few small standard datasets that do not require to download any file from some external website. So, we will use one of them to show how to evaluate the models we build. We will keep life simple and flowery!\n\niris = datasets.load_iris()\n\nNow that we have the dataset, first thing we will do is assign it to target and variables.\n\nx=iris.data\ny=iris.target\nprint(x[0:3])\nprint(y[0:3])\n\n[[5.1 3.5 1.4 0.2]\n [4.9 3.  1.4 0.2]\n [4.7 3.2 1.3 0.2]]\n[0 0 0]\n\n\nNot very descriptive is it? This is perhaps the best known database to be found in the pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. You can find details here : https://scikit-learn.org/stable/datasets/toy_dataset.html\nWe can now find the attributes information\n\nprint(\"The features are: \",list(iris.feature_names))\nprint(\"The target classes are: \", list(iris.target_names))\n\nThe features are:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\nThe target classes are:  ['setosa', 'versicolor', 'virginica']\n\n\nSo, X is an array of the features respectively and y have the class attributes. For example, 0 stands for setosa, 1 stands for versicolor and so on. You get the idea! We will use the features to predict the class of the sample flower. Told you it is going to be flowery!"
  },
  {
    "objectID": "posts/Classification/index.html#training-and-testing-split",
    "href": "posts/Classification/index.html#training-and-testing-split",
    "title": "But how do we evaluate classifiers?",
    "section": "Training and Testing split",
    "text": "Training and Testing split\nAs usual, we need a dataset to teach the model the pattern, the Training Set and another to test how good a student it really is, the Testing Set.\n\nfrom sklearn.model_selection import train_test_split\n# 70% for training and 30% for testing\nX_train, X_test, y_train, y_test = train_test_split(x, y,test_size=.3, random_state=42) #The answer to everythin  in the universe is 42, use it."
  },
  {
    "objectID": "posts/Classification/index.html#lets-fire-up-the-models",
    "href": "posts/Classification/index.html#lets-fire-up-the-models",
    "title": "But how do we evaluate classifiers?",
    "section": "Lets fire up the models",
    "text": "Lets fire up the models\nNow we get to choose the models we want to evaluate, For demonstration purposes, I am choosing 3 models - SGD Classifier, Decision Tree Classifier and Random Forest Classifier. You can choose any models you want but the framework will be same.\n\n#importing the models\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nsgd_model = SGDClassifier(random_state = 42)\ndt_model = DecisionTreeClassifier(random_state = 42)\nrf_model = RandomForestClassifier(random_state = 42)\n\nWe have the models. First, we can use the Accuracy metric to see how all three can perform.\n\nfrom sklearn.metrics import accuracy_score\nmodels = {\n    \"SGD \": sgd_model,\n    \"Decision Tree \": dt_model,\n    \"Random Forest\": rf_model\n}\ndef model_accuracies(models, X_train, y_train, X_test, y_test):\n    for model_name, model in models.items():\n        \n        model.fit(X_train, y_train)   \n        y_pred = model.predict(X_test)\n     \n        accuracy = accuracy_score(y_test, y_pred)\n        \n        print(f\"{model_name}: {accuracy:.2f}\")\nmodel_accuracies(models, X_train, y_train, X_test, y_test)\n\nSGD : 0.80\nDecision Tree : 1.00\nRandom Forest: 1.00\n\n\nWhoa! Would you look at that! But seriously life can not be that good, right? We can not trust the model yet. For a better understanding, we need to look at something called Cross-Validation"
  },
  {
    "objectID": "posts/Classification/index.html#cross-validation",
    "href": "posts/Classification/index.html#cross-validation",
    "title": "But how do we evaluate classifiers?",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nWe split the data into k folds i.e. k number of non overlapping subsets. In this case, lets do 3 folds to keep it simple. Another concept is using a Dummy Classifier. It does not look at the features. In concept, your model has to surpass the dummy classifier to be considered a good model.\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier() \n\ndef cv_score(models,X_train, y_train):\n    for model_name, model in models.items():\n        cv_score = cross_val_score(model,X_train, y_train, cv=3, scoring=\"accuracy\").mean().round(3)\n        print(f\"{model_name}: {cv_score}\")\n        \ncv_score(models,X_train, y_train)\n\nprint(\"Dummy Classifier: \", cross_val_score(dummy_clf ,X_train, y_train, cv=3, scoring=\"accuracy\").mean().round(3))\n\nSGD : 0.924\nDecision Tree : 0.895\nRandom Forest: 0.933\nDummy Classifier:  0.343\n\n\nAlright, now the SGD model does not look too bad, does it? But there is another check we need to do. That is called Confusion Matrix"
  },
  {
    "objectID": "posts/Classification/index.html#confusion-matrix",
    "href": "posts/Classification/index.html#confusion-matrix",
    "title": "But how do we evaluate classifiers?",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\nSimply put, it is a visual tool to undestand how confused the model is when predicting the classes. Or we can say a tool to understand how much the model is misclassifying each classes. We can generate the confusin matrix for each model below-\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nclass_names = list(iris.target_names)\ndef plot_confusion_matrices(models, X_train, y_train, X_test, y_test, class_names):\n\n   \n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n    axes = axes.flatten()  \n    fig.delaxes(axes[3])  \n\n    for i, (model_name, model) in enumerate(models.items()):\n        \n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        \n        sgd_cm = confusion_matrix(y_test, y_pred)\n        \n        sns.heatmap(sgd_cm, annot=True, fmt='d', ax=axes[i], cmap='Blues')\n        axes[i].set_title(model_name)\n        axes[i].set_xlabel('Predicted')\n        axes[i].set_ylabel('Actual')\n        axes[i].set_xticklabels(class_names)\n        axes[i].set_yticklabels(class_names)\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_confusion_matrices(models, X_train, y_train, X_test, y_test, class_names)\n\n\n\n\nAs we can see in the figure, SGD had 8 instances where it misclassified Versicolor as Virginica. So our choice comes to the Decision Tree and Random Forest. Now we did a multiclass classification. Two important metrics are recall and precision. This concept is better understood in binary classfification. We can make our multiclass problem into binary problem by selecting a class. Maybe we can go with the class Versicolor. So, our models are going to predict if an instance represents versicolor or not."
  },
  {
    "objectID": "posts/Classification/index.html#recall-and-precision",
    "href": "posts/Classification/index.html#recall-and-precision",
    "title": "But how do we evaluate classifiers?",
    "section": "Recall and Precision",
    "text": "Recall and Precision\nTo understand recall and precision we need to understand few terms from a binary confusion matrix. So first, we can generate one for SGD after binarizing the data.\n\nfrom sklearn.preprocessing import label_binarize\ny_bin = label_binarize(y, classes=[0, 1, 2])[:, 1]\n\nX_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(x, y_bin,test_size=.3, random_state=42)\n\nsgd_model.fit(X_train_bin, y_train_bin)\ny_pred_bin = sgd_model.predict(X_test_bin)\n\nsgd_cm = confusion_matrix(y_test_bin, y_pred_bin)\n\n# Extracting the values from the confusion matrix\nTP = sgd_cm[1, 1]  # True Positives\nTN = sgd_cm[0, 0]  # True Negatives\nFP = sgd_cm[0, 1]  # False Positives\nFN = sgd_cm[1, 0]  # False Negatives\n\nprint(\"True Positives: \", TP)\nprint(\"True Negatives: \", TN)\nprint(\"False Positives: \", FP)\nprint(\"False Negatives: \", FN)\n\nTrue Positives:  1\nTrue Negatives:  32\nFalse Positives:  0\nFalse Negatives:  12"
  },
  {
    "objectID": "posts/Classification/index.html#key-attributes-of-a-confusion-matrix",
    "href": "posts/Classification/index.html#key-attributes-of-a-confusion-matrix",
    "title": "But how do we evaluate classifiers?",
    "section": "Key Attributes of a Confusion Matrix",
    "text": "Key Attributes of a Confusion Matrix\n\nTrue Positives (TP): These are cases in which the model correctly predicts the positive class. In other words, instances where the actual class is positive, and the model also predicts a positive class.\nTrue Negatives (TN): These are cases where the model correctly predicts the negative class. This means the actual class is negative, and the model also predicts a negative class.\nFalse Positives (FP): These are instances where the model incorrectly predicts the positive class. This happens when the actual class is negative, but the model predicts a positive class. It is also known as a “Type I error”.\nFalse Negatives (FN): These are cases where the model incorrectly predicts the negative class. In these instances, the actual class is positive, but the model predicts a negative class. This is also referred to as a “Type II error”.\n\n\nPrecision\nPrecision is the ratio of correctly predicted positive observations to the total predicted positive observations. It is a measure of a classifier’s exactness. A low precision indicates a high number of false positives. The precision is defined as:\n\\[ \\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \\]\n\nTP (True Positives): The number of positive instances correctly identified by the model.\nFP (False Positives): The number of negative instances incorrectly identified as positive by the moel.\n\n\n\n\nPrecision is particularly important in scenarios where the cost of a false positive is high. For example, in email spam detection, a false positive (non-spam email incorrectly classified as spam) could mean missing an important email.\n\n\nRecall\nRecall, also known as sensitivity, is the ratio of correctly predicted positive observations to all observations in the actual class. It is a measure of a classifier’s completeness. A low recall indicates a high number of false negatives. The recall is defi\\[ \\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \\]\n\nTP (True Positives): The number of positive instances correctly identified by the model.\nFN (False Negatives): The number of positive instances incorrectly identified as negative by the model.egatives.\n\nRecall becomes critical in cases where missing an actual positive is significantly worse than getting false positives. For instance, in medical diagnosis, a false negative (missing a disease) could be much more detrimental the positive.an a fals"
  },
  {
    "objectID": "posts/Classification/index.html#precision-recall-curve",
    "href": "posts/Classification/index.html#precision-recall-curve",
    "title": "But how do we evaluate classifiers?",
    "section": "Precision-Recall Curve",
    "text": "Precision-Recall Curve\nThe Precision-Recall Curve is a plot that illustrates the trade-off between precision and recall for different threshold settings of a classifier. It is used as a tool to select models that balance precision and recall in a desirable way, especially in scenarios with imbalanced datasets.\n\nimport numpy as np\nfrom sklearn.metrics import PrecisionRecallDisplay, precision_recall_curve\nfrom sklearn.model_selection import cross_val_predict\n\ny_probas_dt = cross_val_predict(dt_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"predict_proba\") \ny_scores_dt = y_probas_dt[:, 1]\nprecisions_dt, recalls_dt, thresholds_dt = precision_recall_curve(y_train_bin, y_scores_dt)\n\ny_scores_sgd= cross_val_predict(sgd_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"decision_function\")\n\nprecisions_sgd, recalls_sgd, thresholds_sgd = precision_recall_curve(y_train_bin, y_scores_sgd)\n\ny_probas_rf = cross_val_predict(rf_model, X_train_bin, y_train_bin, cv=3,\n                                    method=\"predict_proba\")\ny_scores_rf = y_probas_rf[:, 1]\nprecisions_rf, recalls_rf, thresholds_rf = precision_recall_curve(y_train_bin, y_scores_rf)\n\n\nplt.figure(figsize=(6, 5))  \n\nplt.plot(recalls_rf, precisions_rf, \"b-\", linewidth=2,\n         label=\"Random Forest\")\nplt.plot(recalls_sgd, precisions_sgd, \"--\", linewidth=2, label=\"SGD\")\nplt.plot(recalls_dt, precisions_dt, \"--\", linewidth=2, label=\"Decision Tree\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.axis([0, 1, 0, 1])\nplt.grid()\nplt.legend(loc=\"lower left\")\n\nplt.show()\n\n\n\n\nNow in a precision recall curve, we look at the top right of the curve. Best model for the purpose will be in that region. That means higher precision with higher recall. The interpretation of the results can be explaines as-\n\nRandom Forest (solid blue line): This model has the best performance among the three. It maintains a high precision even as recall increases, which means it correctly identifies a high percentage of positive cases and, when it predicts a case to be positive, it is likely correct.\nSGD (dashed line): The performance of the SGD model varies more than the Random Forest. Its precision starts lower and fluctuates as recall increases. This suggests that the model may not be as reliable in its positive predictions compared to the Random Forest.\nDecision Tree (dashed green line): The Decision Tree’s performance is generally lower than the Random Forest. While it has moments where its precision is high, it also has points where precision drops significantly as recall increases. This might indicate that the Decision Tree model, at certain thresholds, predicts many false positives while trying to capture more true positives.\n\nSo there we have it, Random Forest can be used for the job at hand! Thank you for reading the blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blogs",
    "section": "",
    "text": "All About Anomaly in Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBut How Do We Evaluate Classifiers?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nIntroduction to Unsupervised Machine Learning Through Clustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMechanism of Regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWonderful World of Probability\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nSiam Maksud\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, Welcome to my website."
  },
  {
    "objectID": "posts/Anomaly-Detection/index.html",
    "href": "posts/Anomaly-Detection/index.html",
    "title": "All About Anomaly in Data",
    "section": "",
    "text": "In this age where data is king, we are awash in a sea of big data, larger and more complex than ever before. Within this vast expanse of information lies a subtle yet critical challenge: the presence of anomalies. Think of anomalies as the rebels of the data world, defying the norms and expectations of typical behavior. These aren’t just quirky data points; they’re potential game-changers. They can skew our insights, open doors to cyber threats, and lead our carefully calibrated models astray. Identifying and understanding these anomalies isn’t just a technical puzzle; it’s a crucial quest to safeguard the integrity and efficacy of our data-driven universe. The ability to pinpoint and interpret these unusual patterns is more than a skill—it’s an essential armor in the arsenal of any data warrior navigating this ever-expanding digital landscape..\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(0)\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, 100)\n\n# Introducing outliers/anomalies\ny[np.random.randint(0, len(y), 5)] += np.random.normal(3, 1, 5)\n\n# Identifying the anomalies\nthreshold = y.mean() + 1 * y.std()\nanomalies = y &gt; threshold\n\ndata = np.column_stack((x, y))\ndf = pd.DataFrame(data, columns=['X', 'Y'])\ndf['Anomaly'] = anomalies\n\n# Plotting \nplt.figure(figsize=(8, 5))\nsns.scatterplot(x='X', y='Y', hue='Anomaly', data=df, palette={False: 'blue', True: 'red'})\n\nplt.axhline(y=threshold, color='green', linestyle='--', label='Threshold')\nplt.xlabel('X-axis')\nplt.ylabel('Y-axis')\nplt.grid()\nplt.title('Data Points with Anomalies Highlighted (Seaborn)')\nplt.legend()\nplt.show()\nThe plot showcases how a general anomaly can look like in a data distribution, these anomalies are separated by a threshold. If you look at the plot, it can be very evident that the outliers does not follow the actual pattern."
  },
  {
    "objectID": "posts/Anomaly-Detection/index.html#machine-learning-in-anomaly-detection",
    "href": "posts/Anomaly-Detection/index.html#machine-learning-in-anomaly-detection",
    "title": "All About Anomaly in Data",
    "section": "Machine Learning in Anomaly Detection",
    "text": "Machine Learning in Anomaly Detection\nMachine learning excels in identifying patterns within datasets, a capability that proves especially useful in anomaly detection. By mastering the art of pattern recognition, machine learning models are not only able to discern regularities in data but also adeptly pinpoint anomalies—those instances that deviate from established norms. Today, I’ll introduce you to a particularly effective model designed for this purpose. This model simplifies the complex task of detecting outliers, leveraging advanced algorithms to efficiently identify deviations in any dataset, thus offering a robust solution for a variety of anomaly detection challenges."
  },
  {
    "objectID": "posts/Anomaly-Detection/index.html#dbscan",
    "href": "posts/Anomaly-Detection/index.html#dbscan",
    "title": "All About Anomaly in Data",
    "section": "DBSCAN",
    "text": "DBSCAN\nDBSCAN is a density based clustering algorithm which is really good for finding arbitary shaped clusters. Imagine you’re a detective looking at a bustling crowd, trying to identify groups of friends based on how closely they stand together. This is similar to how DBSCAN works with data points.\nWe will apply this algorithm to find out anomaly in the IRIS dataset. It has many repositories but the one from UCI Machine Learning Repository has two wrong data points.\nsource: https://archive.ics.uci.edu/static/public/53/iris.zip\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\")\nprint(df.head(-4))\n\n     sepal_length  sepal_width  petal_length  petal_width    species\n0             5.1          3.5           1.4          0.2     setosa\n1             4.9          3.0           1.4          0.2     setosa\n2             4.7          3.2           1.3          0.2     setosa\n3             4.6          3.1           1.5          0.2     setosa\n4             5.0          3.6           1.4          0.2     setosa\n..            ...          ...           ...          ...        ...\n141           6.9          3.1           5.1          2.3  virginica\n142           5.8          2.7           5.1          1.9  virginica\n143           6.8          3.2           5.9          2.3  virginica\n144           6.7          3.3           5.7          2.5  virginica\n145           6.7          3.0           5.2          2.3  virginica\n\n[146 rows x 5 columns]\n\n\nLets perform a simple model fitting with petal length and petal width\n\nfrom sklearn.cluster import DBSCAN\nmodel=DBSCAN()\nmodel.fit(df[[\"sepal_length\", \"sepal_width\"]])\\\n\nmodel.get_params(deep=True)\n\n{'algorithm': 'auto',\n 'eps': 0.5,\n 'leaf_size': 30,\n 'metric': 'euclidean',\n 'metric_params': None,\n 'min_samples': 5,\n 'n_jobs': None,\n 'p': None}\n\n\nWe need to look at one parameter called epsilon (ε). Epsilon is like the detective’s line of sight, determining how far he can see around each point. Here we have ε = 0.5\nFirst, maybe we can see if the model can really detect the two wrong data points. We can call them outliers or data anomaly.\n\nlabels = model.labels_\nprint(pd.Series(labels).value_counts())\n\n 0    148\n-1      2\ndtype: int64\n\n\nAs we can see, the model really could find out two wrong data points. You will see the DBSCAN model labelled them as -1. Just to be sure, we can plot the labels. In the image you can see our two outliers colored orange dots.\n\ndf['Outlier'] = labels == -1\n\nplt.figure(figsize=(6, 5))\nsns.scatterplot(data=df, x=\"sepal_length\", y=\"sepal_width\", hue=\"Outlier\",s = 50, palette=\"deep\",legend=False)\nplt.title(\"DBSCAN Outlier Detection\")\nplt.xlabel(\"Sepal Length\")\nplt.ylabel(\"Sepal Width\")\nplt.grid()\nplt.show()\n\n\n\n\nNow, the performance of the model can vary with different epsilon values. That is why, when trying to detect anomalies in your data, always look at the graphical representation of data clusters and anomalies to judge the model performance. Following is a plot showing the model output on three different epsilon values. We can compare the results because we know there are two outliers. As we can see, using ε = 0.1 and 0.7 gave us not ideal results. So, use your judgement always. Good Luck!\n\ndef plot_dbscan(df, eps_values):\n    plt.figure(figsize=(10,3))\n\n    for i, eps in enumerate(eps_values, 1):\n        model = DBSCAN(eps=eps)\n        df['labels'] = model.fit_predict(df[[\"sepal_length\", \"sepal_width\"]])\n\n        plt.subplot(1, len(eps_values), i)\n        sns.scatterplot(data=df, x=\"sepal_length\", y=\"sepal_width\", hue=\"labels\", palette=\"deep\", legend=False)\n        plt.title(f\"DBSCAN with eps={eps}\")\n        plt.xlabel(\"Sepal Length\")\n        plt.ylabel(\"Sepal Width\")\n\n    plt.tight_layout()\n    plt.show()\n\n\neps_values = [0.1, 0.5, 0.7]\n\n\nplot_dbscan(df, eps_values)"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "",
    "text": "You are on your first day at your job and have just been handed a huge amount of data about your customers. You have been tasked to identify the buying groups your company can direct the advertisement of products to. “Well, that is a mammoth task”-you say to yourself wondering how you can do it. Not to worry, you are familiar with machine learning. You can let an algorithm identify them for you. This task is called Clustering and our friend unsupervised machine learning is here to help you with that. In clustering, the goal is to group similar instances into clusters."
  },
  {
    "objectID": "posts/Clustering/index.html#so-what-is-clustering-anyway",
    "href": "posts/Clustering/index.html#so-what-is-clustering-anyway",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "",
    "text": "You are on your first day at your job and have just been handed a huge amount of data about your customers. You have been tasked to identify the buying groups your company can direct the advertisement of products to. “Well, that is a mammoth task”-you say to yourself wondering how you can do it. Not to worry, you are familiar with machine learning. You can let an algorithm identify them for you. This task is called Clustering and our friend unsupervised machine learning is here to help you with that. In clustering, the goal is to group similar instances into clusters."
  },
  {
    "objectID": "posts/Clustering/index.html#the-game-plan",
    "href": "posts/Clustering/index.html#the-game-plan",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "The Game Plan",
    "text": "The Game Plan\nWe look at the IRIS dataset. It is a comprehensive dataset with 3 classes. We can do a simple classification problem. But let’s go another way. We will forget that the data is labeled. By clustering, we will see if our machine learning model can by itself create the classes. So, until the end, we are going in blind. And the interesting thing about going in blind, we do not have to do data splitting because there will be no traditional training and testing.\n\nimport sklearn\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\niris = datasets.load_iris(as_frame = True)\nX = iris.data\ny = iris.target\n\n\n#plotting the classes\ndata = X.copy()\ndata['Cluster'] = y\n\n# Mapping numerical labels to actual species names\nspecies_map = {0: 'Iris setosa', 1: 'Iris versicolor', 2: 'Iris virginica'}\ndata['Cluster'] = data['Cluster'].map(species_map)\n\n# Plotting\nplt.figure(figsize=(7, 5))\nsns.scatterplot(data=data, x='petal length (cm)', y='petal width (cm)', hue='Cluster',  style='Cluster', markers=['o', 's', '^'])\nplt.title(\"Clustering based on labels\")\nplt.grid(True)\nplt.legend(title='Species')\nplt.show()\n\n\n\n\nThis is what the plot scattered plot looks like in a two din=mensional space. I am only measuring in terms of petal length and width with the assumption that these two features will make the division more clear. And we can see the natural clustering with known labels in the plot."
  },
  {
    "objectID": "posts/Clustering/index.html#k-means-clusturing",
    "href": "posts/Clustering/index.html#k-means-clusturing",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "K-Means Clusturing",
    "text": "K-Means Clusturing\nImagine you want to group a collection of points (or data points) into clusters. Each cluster should have a central point known as a “centroid.” The goal is to ensure each point in a cluster is close to its centroid. The challenge is that you initially don’t know where the centroids should be or which points belong to which cluster.\nSteps to Cluster Data\n1. Start by Guessing:\n- Begin by making a guess about where the centroids might be. - You can do this randomly, such as by picking a few points from your data and using them as your initial centroids.\n2. Assign Points to the Nearest Centroid:\n- Examine each point in your data. - Assign it to the cluster of the closest centroid you guessed. - This forms your initial clusters.\n3. Update Centroids:\n- Once you have your initial clusters, find the new center for each cluster. - The new center is the average position of all points in the cluster. - These average positions become your new centroids.\n4. Repeat:\n- With these new centroids, reassign each point to the closest centroid. - This step may shuffle some points between clusters.\n5. Keep Going Until Stabilization:\n- Continue updating the centroids and reassigning points. - Stop the process when the centroids do not change significantly anymore. - At this point, you have found a stable set of clusters.\nThis method is a basic implementation of a popular clustering algorithm known as “K-Means.” It’s a straightforward yet effective way to group data points into clusters when you don’t have predefined labels or categories. The algorithm is designed to converge to a solution in a reasonable amount of time, ensuring each iteration improves the clustering."
  },
  {
    "objectID": "posts/Clustering/index.html#k-in-k-means-clusturing-why-do-we-need-to-optimize",
    "href": "posts/Clustering/index.html#k-in-k-means-clusturing-why-do-we-need-to-optimize",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "‘K’ in K-means Clusturing: Why Do We Need to Optimize?",
    "text": "‘K’ in K-means Clusturing: Why Do We Need to Optimize?\nIn K-means clusturing, we have to first set the number of clusters to be made during the initialization of the model. If we make too many clusters, the computatinal time will increase and too few will not be useful. But we have no metrics to test the data against real labels so how do we do that?."
  },
  {
    "objectID": "posts/Clustering/index.html#inertia",
    "href": "posts/Clustering/index.html#inertia",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "Inertia",
    "text": "Inertia\nInertial is defined as the the sum of the squared distances between the instances and their closest centroids. So lets run a loop and produce the inertia for different number of clusters.\n\nfrom sklearn.cluster import KMeans\nfrom timeit import timeit\n\nmax_k = 10\ndata = {\n    'k': [],\n    'Inertia': [],\n    'Training Time': []\n}\n\nfor k in range(1, max_k + 1):\n    kmeans_ = KMeans(n_clusters=k, algorithm=\"full\", random_state=42)\n    print(f\"\\r{k}/{max_k}\", end=\"\")  # \\r returns to the start of line\n    training_time = timeit(\"kmeans_.fit(X)\", number=10, globals=globals())\n    inertia = kmeans_.inertia_\n\n    data['k'].append(k)\n    data['Inertia'].append(inertia)\n    data['Training Time'].append(training_time)\n\ndf = pd.DataFrame(data)\n\nsns.set_style(\"whitegrid\")\n\nplt.figure(figsize=(8, 4))\n\n# Inertia plot\nplt.subplot(121)\nsns.lineplot(x='k', y='Inertia', data=df, marker='o', color='red', label=\"K-Means\")\nplt.title(\"Inertia vs. k\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\n# Training time plot\nplt.subplot(122)\nsns.lineplot(x='k', y='Training Time', data=df, marker='o', color='blue', label=\"K-Means\")\nplt.title(\"Training Time vs. k\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Training Time (seconds)\")\n\nplt.tight_layout()\nplt.show()\n\n1/102/103/104/105/106/107/108/109/1010/10\n\n\n\n\n\nAs we can see from the plots, the inertial decreases with increased clusters. That is a good thing! But look the training time. This is increasing much more than we can take. Why? Remember your model will not be scalable if it takes up too much resources. But maybe we can find some way, When we plot inertia as a function of k, after some point, we see the gradient not being very high i.e. inertia is not decreasing that much. This point can look like an elbow, and the elbow joint can lead us to the optimum clusters. Maybe not fullproof but this is a step in the right direction. This method is evidently called Elbow Method.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\n\n\nsns.set(style=\"whitegrid\")\n\nplt.figure(figsize=(6, 3.5))\nplt.plot(range(1, 10), inertias, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Inertia\")\n\nplt.axis([1, 9, 0, max(inertias) + 100])  # Adjusted the axis for better visibility\nplt.title(\"Elbow Method for Optimal $k$\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nCan you notice the elbow like joint when k = 2? maybe 3? now we can identify the region but still there is another good metrics to find out."
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-score",
    "href": "posts/Clustering/index.html#silhouette-score",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "Silhouette Score",
    "text": "Silhouette Score\nAn instance’s silhouette coefficient is equal to \\[\\frac{b - a}{\\max(a, b)}\\] where a is the mean distance to the other instances in the same cluster (i.e., the mean intra-cluster distance) and b is the mean nearest-cluster distance (i.e., the mean distance to the instances of the next closest cluster, defined as the one that minimizes b, excluding the instance’s own cluster).\nThe silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]\n\nplt.figure(figsize=(6, 3.5))\nplt.plot(range(2, 10), silhouette_scores, \"bo-\")\nplt.xlabel(\"$k$\")\nplt.ylabel(\"Silhouette score\")\n\nplt.grid(True, which='both', linestyle='-', linewidth=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\nso when k=2, the silhoutte score is highest. Maybe then the data can be divided into two clusters. But hold your horses. We have one more thing to do. We need to see the Silhoutte Diagram!"
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-diagram",
    "href": "posts/Clustering/index.html#silhouette-diagram",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "Silhouette Diagram",
    "text": "Silhouette Diagram\nA more detailed visual representation can be achieved by plotting the silhouette coefficient of each instance. These plots are organized by the cluster assignment and the coefficient values. Such visualizations are known as silhouette diagrams. Each cluster in the diagram is represented by a ‘knife-like’ shape, where the height shows how many instances are in the cluster, and the width indicates the silhouette coefficients of the instances, sorted within the cluster. In these diagrams, wider shapes are indicative of better clustering.\nThe average silhouette score for each potential number of clusters is marked by vertical dashed lines in the diagram. If the silhouette coefficients of most instances in a cluster are less than this average score, meaning that the knife shapes fall short of the dashed line, it suggests that the cluster is not well-defined. This happens when the instances are too close to instances in other clusters, implying a less than optimal clustering arrangement.\n\nfrom sklearn.metrics import silhouette_samples\nfrom matplotlib.ticker import FixedLocator, FixedFormatter\n\nplt.figure(figsize=(8, 6))\n\nfor k in (2,3, 4, 5):\n    plt.subplot(2, 2, k - 1)\n    \n    y_pred = kmeans_per_k[k - 1].labels_\n    silhouette_coefficients = silhouette_samples(X, y_pred)\n\n    padding = len(X) // 30\n    pos = padding\n    ticks = []\n    for i in range(k):\n        coeffs = silhouette_coefficients[y_pred == i]\n        coeffs.sort()\n\n        color = plt.cm.Spectral(i / k)\n        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n        ticks.append(pos + len(coeffs) // 2)\n        pos += len(coeffs) + padding\n\n    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n    if k in (2, 4):\n        plt.ylabel(\"Cluster\")\n    \n    if k in (4, 6):\n        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n        plt.xlabel(\"Silhouette Coefficient\")\n    else:\n        plt.tick_params(labelbottom=False)\n\n    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n    plt.title(f\"$k={k}$\")\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#interpetting-the-silhoutte-diagram",
    "href": "posts/Clustering/index.html#interpetting-the-silhoutte-diagram",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "Interpetting the Silhoutte Diagram",
    "text": "Interpetting the Silhoutte Diagram\n\nFor k=2: This setup shows two clusters with a fairly good silhouette width for the larger cluster. This suggests a decent separation between clusters. However, the simplicity of having only two clusters might not capture all the nuances in the data.\nFor k=3: Increasing the cluster count to three shows more uniformity in silhouette widths, indicating that the clusters might be more distinct and appropriately separated. Yet, some instances below the average silhouette score signal potential overlap between clusters.\nFor k=4: Here, the silhouette widths start to vary more significantly, implying different levels of cohesion within the clusters. The presence of lower silhouette scores within some clusters indicates potential misplacements or that these clusters are not capturing a distinct grouping in the data.\nFor k=5: With five clusters, we notice even more variation in silhouette widths, with generally narrower shapes compared to k=3. This suggests that the additional cluster may not be necessary as it does not seem to capture distinct groupings, and some points could potentially belong to other clusters.\n\nBased on this analysis, k=3 may be the most appropriate number of clusters for this dataset, as the clusters display a balance of separation and uniformity in silhouette widths. As the number of clusters increases to 4 and 5, the silhouette diagrams indicate a diminishing return in cluster definition and separation."
  },
  {
    "objectID": "posts/Clustering/index.html#overall-comparison-with-the-actual-labels",
    "href": "posts/Clustering/index.html#overall-comparison-with-the-actual-labels",
    "title": "Introduction to Unsupervised Machine Learning Through Clustering",
    "section": "Overall Comparison With the Actual Labels",
    "text": "Overall Comparison With the Actual Labels\nFinally we can now create three 2D spaces for actual labels and see how the clusters show the real labels. We are doing this just because we pretended we do not have actual labels so now we can compare!\n\nfrom scipy import stats\n\nkmeans = KMeans(n_clusters=3, random_state=42)\ny_pred = kmeans.fit_predict(X)\n\n# Create a mapping based on modes\nmapping = {}\nfor class_id in np.unique(y):\n    mode, _ = stats.mode(y_pred[y == class_id])\n    if np.isscalar(mode):\n        mapping_key = mode\n    else:\n        mapping_key = mode[0] if mode.size &gt; 0 else -1\n    mapping[mapping_key] = class_id\n\ny_pred = np.array([mapping.get(cluster_id, -1) for cluster_id in y_pred])\n\ndata = X.copy()\ndata['Predicted'] = y_pred\ndata['Actual'] = y\n\nlabel_names = {0: 'Iris setosa', 1: 'Iris versicolor', 2: 'Iris virginica'}\ndata['Actual Names'] = data['Actual'].map(label_names)\n\n# Plotting using FacetGrid\nplt.figure(figsize=(6, 4))\ng = sns.FacetGrid(data, col=\"Actual Names\", hue=\"Predicted\", palette=\"bright\", height=4, aspect=1)\ng.map(plt.scatter, 'petal length (cm)', 'petal width (cm)', alpha=0.6, s=100)\ng.add_legend()\ng.set_titles(\"{col_name}\")\n\nfor ax in g.axes.flatten():\n    ax.grid(True)  \n    ax.set_xlabel(\"Petal length\")\n    ax.set_ylabel(\"Petal width\")\n\nplt.show()\n\n&lt;Figure size 576x384 with 0 Axes&gt;\n\n\n\n\n\nSo our dear to heart K-Means model did made some error. But remember “To err is human”. See what I did there? Good luck!"
  },
  {
    "objectID": "posts/Probability-And-Random-Variables/index.html",
    "href": "posts/Probability-And-Random-Variables/index.html",
    "title": "Wonderful World of Probability",
    "section": "",
    "text": "In our day to day life we have to make random decisions. For that we decide if one even is likely to occur. This likelihood is called probability. Now if I say how much is the probability of getting Head or Tail in a coin toss the math will tell us it has a 50/50 chance. But that rarely happens when you do a coin toss mulitple times. The magic is when you do it for a large number or times, the chances goes closer and closer to the mathematical probability. That is the magic happening in real time with large number of events."
  },
  {
    "objectID": "posts/Probability-And-Random-Variables/index.html#introducing-bayes-theorem",
    "href": "posts/Probability-And-Random-Variables/index.html#introducing-bayes-theorem",
    "title": "Wonderful World of Probability",
    "section": "Introducing Bayes’ Theorem",
    "text": "Introducing Bayes’ Theorem\nBayes’ Theorem helps us understand how the probability of an event changes as we gain more information.\nMathematical Representation Bayes’ Theorem is mathematically expressed as:\n\\[\nP(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)}\n\\]\nWhere:\n\n\\(P(A | B)\\) is the probability of event A occurring given that B is true.\n\\(P(B | A)\\) is the probability of event B occurring given that A is true.\n\\(P(A)\\) is the probability of event A.\n\\(P(B)\\) is the probability of event B."
  },
  {
    "objectID": "posts/Probability-And-Random-Variables/index.html#bayes-theorem-in-action",
    "href": "posts/Probability-And-Random-Variables/index.html#bayes-theorem-in-action",
    "title": "Wonderful World of Probability",
    "section": "Bayes’ Theorem in Action!",
    "text": "Bayes’ Theorem in Action!"
  },
  {
    "objectID": "posts/Probability-And-Random-Variables/index.html#probability-density-function-pdf",
    "href": "posts/Probability-And-Random-Variables/index.html#probability-density-function-pdf",
    "title": "Wonderful World of Probability",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\nA PDF is a function that describes the relative likelihood for a continuous random variable to take on a given value. In simpler terms, it shows how the values of a continuous variable are distributed.\nIn a PDF, the y-axis represents the density, not the probability. For continuous random variables, the probability of observing any single exact value is essentially zero because there are infinitely many possible values. Instead, the area under the curve of the PDF within a range of values indicates the probability of the variable falling within that range.The total area under the PDF curve equals 1, representing the total probability space.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6, 3.5))\n\n# Plot for all penguins\nsns.histplot(all_flipper_data, kde=True, stat=\"density\", label='All Penguins')\n# Plot for Chinstrap penguins\nsns.histplot(chinstrap_data, kde=True, stat=\"density\", color='salmon', label='Chinstrap Penguins')\n\nplt.axvline(x=flipper_length, color='red', linestyle='--')\nplt.text(flipper_length+1, 0.02, f'Flipper Length {flipper_length}mm', rotation=90, color='red')\n\nplt.title('Probability Density Function of Flipper Lengths')\nplt.xlabel('Flipper Length (mm)')\nplt.ylabel('Density')\nplt.legend()\n\nplt.show()\n\n\n\n\nWhen dealing with continuous data (like flipper length in penguins), the concepts of prior, likelihood, and posterior in Bayes’ Theorem are represented as continuous probability distributions, and these are often expressed as PDFs.\nIn our penguin dataset, we used Bayes’ Theorem to update the probability of a penguin being of the Chinstrap species given its flipper length. The prior probability (belief about the proportion of Chinstrap penguins before seeing the flipper length), likelihood (probability of observing a certain flipper length given that the penguin is a Chinstrap), and the marginal probability (overall probability of observing this flipper length) are all part of this Bayesian update mechanism.\nThe PDFs represent these probabilities for the continuous variable of flipper length. By calculating the area under these curves (or using a probability density function to get a specific density value), we are able to apply Bayes’ Theorem in a context where the variables are continuous."
  }
]